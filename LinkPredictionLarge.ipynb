{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import networkx as nx\n",
    "from sentence_transformers import util\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import Dataset\n",
    "#import torch\n",
    "\n",
    "PAPERNUM = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('withRef.csv', nrows=PAPERNUM)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize graph and add nodes\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(df['id'].astype(str).tolist())\n",
    "df['references'] = df['references'].astype(str).apply(lambda x: x.strip().split(';') if x else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building graph\"):\n",
    "    for ref in row['references']:\n",
    "        if ref in G:\n",
    "            G.add_edge(row['id'], ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph shape\n",
    "print(len(G.nodes))\n",
    "print(len(G.edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a fraction of edges for testing\n",
    "def train_test_split_graph(G, test_ratio=0.3, seed=52):\n",
    "    random.seed(seed)\n",
    "    edges = list(G.edges())\n",
    "    num_test = int(len(edges) * test_ratio)\n",
    "    test_edges = random.sample(edges, num_test)\n",
    "    train_graph = G.copy()\n",
    "    train_graph.remove_edges_from(test_edges)\n",
    "    return train_graph, test_edges\n",
    "\n",
    "G_train, test_edges = train_test_split_graph(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random node pairs that are not connected to evaluate false positives\n",
    "def generate_negative_edges(G, num_edges, excluded_edges):\n",
    "    nodes = list(G.nodes())\n",
    "    neg_edges = set()\n",
    "    while len(neg_edges) < num_edges:\n",
    "        u, v = random.sample(nodes, 2)\n",
    "        if not G.has_edge(u, v) and (u, v) not in excluded_edges:\n",
    "            neg_edges.add((u, v))\n",
    "    return list(neg_edges)\n",
    "\n",
    "negative_edges = generate_negative_edges(G, len(test_edges), set(test_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to undirected graphs\n",
    "G_undirected = G.to_undirected()\n",
    "G_train_undirected = G_train.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Neighbors\n",
    "pred_common = [\n",
    "    (u, v, len(list(nx.common_neighbors(G_train_undirected, u, v))))\n",
    "    for u, v in test_edges + negative_edges\n",
    "    if u in G_train_undirected and v in G_train_undirected and len(list(nx.common_neighbors(G_train_undirected, u, v))) > 0\n",
    "]\n",
    "print(f\"Number of Common Neighbors: {len(pred_common)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard Coefficient\n",
    "neighbors = {node: set(G_train_undirected.neighbors(node)) for node in G_train_undirected.nodes()}\n",
    "\n",
    "def fast_jaccard(u, v):\n",
    "    if u in neighbors and v in neighbors:\n",
    "        inter = neighbors[u] & neighbors[v]\n",
    "        union = neighbors[u] | neighbors[v]\n",
    "        return (u, v, len(inter) / len(union)) if union else (u, v, 0.0)\n",
    "    return (u, v, 0.0)\n",
    "\n",
    "pred_jaccard = [fast_jaccard(u, v) for u, v in tqdm(test_edges + negative_edges, desc=\"Calculating Jaccard Coefficient\", total=len(test_edges) + len(negative_edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamic-Adar Index\n",
    "pred_adamic = list(nx.adamic_adar_index(G_train_undirected, test_edges + negative_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(preds, true_edges_set):\n",
    "    y_true = [(u, v) in true_edges_set for u, v, _ in preds]\n",
    "    y_scores = [score for _, _, score in preds]\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "true_set = set(test_edges)\n",
    "auc_jaccard = evaluate_predictions(pred_jaccard, true_set)\n",
    "auc_adamic = evaluate_predictions(pred_adamic, true_set)\n",
    "\n",
    "# AUC scores\n",
    "print(f\"AUC - Jaccard: {auc_jaccard:.4f}\")\n",
    "print(f\"AUC - Adamic-Adar: {auc_adamic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction With LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['id'].astype(str)\n",
    "hf_dataset = Dataset.from_pandas(df.filter(['id', 'title', 'references'], axis=1))\n",
    "\n",
    "# Remove unnecessary index column (automatically added by from_pandas)\n",
    "hf_dataset = hf_dataset.remove_columns([\"__index_level_0__\"]) if \"__index_level_0__\" in hf_dataset.column_names else hf_dataset\n",
    "\n",
    "# Verify dataset structure\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def generate_all_embeddings(batch):\n",
    "    texts = batch[\"title\"]\n",
    "    embeddings = embedding_model.embed_documents(texts)\n",
    "    return {\"embedding\": embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 256\n",
    "num_batches = len(hf_dataset) // BATCH_SIZE + (len(hf_dataset) % BATCH_SIZE > 0)\n",
    "\n",
    "# Batched processing with tqdm\n",
    "batched_embeddings = []\n",
    "\n",
    "for i in tqdm(range(num_batches), desc=\"Embedding batches\"):\n",
    "    start = i * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, len(hf_dataset))\n",
    "    batch = hf_dataset[start:end]\n",
    "    \n",
    "    embeddings = generate_all_embeddings(batch)\n",
    "    batched_embeddings.extend(embeddings[\"embedding\"])\n",
    "\n",
    "\n",
    "# Add embeddings to the dataset\n",
    "#hf_dataset = hf_dataset.add_column(\"embedding\", batched_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_embeddings = np.array(batched_embeddings, dtype=np.float32)\n",
    "\n",
    "node_embeddings = {\n",
    "    str(hf_dataset[i][\"id\"]): np.array(embedding, dtype=np.float32)\n",
    "    for i, embedding in tqdm(enumerate(batched_embeddings), total=len(batched_embeddings), desc=\"Building node_embeddings\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity from embeddings\n",
    "def cached_score(u, v):\n",
    "    if u in node_embeddings and v in node_embeddings:\n",
    "        return util.cos_sim(node_embeddings[u], node_embeddings[v]).item()\n",
    "    return 0.5  # fallback for missing nodes\n",
    "\n",
    "# Generate predictions\n",
    "llm_preds = [(u, v, cached_score(u, v)) for u, v in tqdm(test_edges + negative_edges, desc=\"Calculating LLM scores\", total=len(test_edges) + len(negative_edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC scores\n",
    "auc_sbert = evaluate_predictions(llm_preds, set(test_edges))\n",
    "print(f\"AUC - SBERT: {auc_sbert:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
